{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-06T08:27:15.220369Z","iopub.status.busy":"2024-05-06T08:27:15.219791Z","iopub.status.idle":"2024-05-06T08:27:32.446329Z","shell.execute_reply":"2024-05-06T08:27:32.445327Z","shell.execute_reply.started":"2024-05-06T08:27:15.220331Z"},"trusted":true},"outputs":[],"source":["from utils.dataset import HumanEvalDataset\n","data_root ='data/'\n","language = 'python'\n","dataset = HumanEvalDataset(data_root, language=language)\n"]},{"cell_type":"markdown","metadata":{},"source":["GENERATE CODE"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-05-06T08:27:32.457156Z","iopub.status.busy":"2024-05-06T08:27:32.456866Z","iopub.status.idle":"2024-05-06T08:27:35.650287Z","shell.execute_reply":"2024-05-06T08:27:35.649164Z","shell.execute_reply.started":"2024-05-06T08:27:32.457133Z"},"trusted":true},"outputs":[],"source":["import glob \n","import json\n","import os\n","for file in glob.glob('human_eval\\\\open_ai_call\\\\output_*.jsonl'):\n","    source_file = file.replace('output_','')\n","    mapping_file = source_file +\".mapping.json\"\n","    data = list()\n","    if os.path.exists(mapping_file):\n","        with open(mapping_file) as mp_file:\n","            mapping = json.load(mp_file)\n","    else:\n","        mapping = dict()\n","    with open(file) as f:\n","        for l in f.readlines():\n","            obj = json.loads(l.strip())\n","            if obj['custom_id'].isnumeric():\n","                obj['custom_id'] = mapping[obj['custom_id']]\n","            data.append(obj)\n","    with open(file,'w+') as f:\n","        for obj in data:\n","            f.writelines(json.dumps(obj)+'\\n')\n","    # print(file)\n","    # print(data[0])"]},{"cell_type":"markdown","metadata":{},"source":["**merge**"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["human_eval/open_ai_call/output_mini_batch_final_java_control_dep.jsonl\n","human_eval/open_ai_call/output_mini_batch_final_java_data_dep.jsonl\n","human_eval/open_ai_call/output_mini_batch_final_java_method_name.jsonl\n","human_eval/open_ai_call/output_mini_batch_final_java_output.jsonl\n","human_eval/open_ai_call/output_mini_batch_final_java_summarize.jsonl\n","human_eval/open_ai_call/output_mini_batch_final_python_control_dep.jsonl\n","human_eval/open_ai_call/output_mini_batch_final_python_data_dep.jsonl\n","human_eval/open_ai_call/output_mini_batch_final_python_method_name.jsonl\n","human_eval/open_ai_call/output_mini_batch_final_python_output.jsonl\n","human_eval/open_ai_call/output_mini_batch_final_python_summarize.jsonl\n"]}],"source":["temps = [\n","    'human_eval/open_ai_call/output_mini_batch_*_java_control_dep.jsonl',\n","'human_eval/open_ai_call/output_mini_batch_*_java_data_dep.jsonl',\n","'human_eval/open_ai_call/output_mini_batch_*_java_method_name.jsonl',\n","'human_eval/open_ai_call/output_mini_batch_*_java_output.jsonl',\n","'human_eval/open_ai_call/output_mini_batch_*_java_summarize.jsonl',\n","'human_eval/open_ai_call/output_mini_batch_*_python_control_dep.jsonl',\n","'human_eval/open_ai_call/output_mini_batch_*_python_data_dep.jsonl',\n","'human_eval/open_ai_call/output_mini_batch_*_python_method_name.jsonl',\n","'human_eval/open_ai_call/output_mini_batch_*_python_output.jsonl',\n","'human_eval/open_ai_call/output_mini_batch_*_python_summarize.jsonl',\n","]\n","for temp in temps:\n","    out = temp.replace('*','final')\n","    data = list()\n","    for file in glob.glob(temp):\n","        with open(file) as f:\n","            for l in f.readlines():\n","                obj = json.loads(l.strip())\n","                data.append(obj)\n","    with open(out,'w+') as f:\n","        for obj in data:\n","            f.writelines(json.dumps(obj)+'\\n')\n","    print(out)"]},{"cell_type":"markdown","metadata":{},"source":["GERERATE DEPENDENCE"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'task_id': 'HumanEval_23_strlen', 'response': 'There are no pairs of statements in the given source code that have a data dependence relationship.'}\n","{'task_id': 'HumanEval_23_strlen', 'response': 'There are no pairs of statements in the given source code that have a control dependence relationship.'}\n","{'task_id': 'Python/0', 'response': '1. Line 5 depends on line 4\\n2. Line 6 depends on line 4\\n3. Line 6 depends on line 5\\n4. Line 7 depends on line 6'}\n","{'task_id': 'Python/0', 'response': '1. Line 3 and Line 5\\n2. Line 5 and Line 7'}\n"]}],"source":["# xxx\n","import pandas as pd \n","import json \n","lang = 'java'\n","type_dep = 'control_dependence'\n","\n","def parser_jsonl(file):\n","    result = list()\n","    with open(file) as f:\n","        for l in f.readlines():\n","            obj = json.loads(l.strip())\n","            res = obj['response']['body']['choices'][0]['message']['content']\n","            custom_id = obj['custom_id']\n","            task_id = custom_id.split(\"#####\")[0]\n","            result.append({'task_id':task_id,'response':res})\n","    return result\n","\n","# human_eval/open_ai_call/output_mini_batch_final_java_control_dep.jsonl\n","# human_eval/open_ai_call/output_mini_batch_final_java_data_dep.jsonl\n","# human_eval/open_ai_call/output_mini_batch_final_java_method_name.jsonl\n","# human_eval/open_ai_call/output_mini_batch_final_java_output.jsonl\n","# human_eval/open_ai_call/output_mini_batch_final_java_summarize.jsonl  \n","import pandas as pd\n","langs = ['java','python']\n","deps = ['data_dep','control_dep']     \n","for lang in langs:\n","    for dep in deps:\n","        file = f\"human_eval/open_ai_call/output_mini_batch_final_{lang}_{dep}.jsonl\"\n","        data = parser_jsonl(file)\n","        print(data[0])\n","        df = pd.DataFrame(data)\n","        df.to_csv(f'human_eval/{lang}/{dep}_{lang}.csv',index=False)\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["GENERATE NAME PRD"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["human_eval/open_ai_call/output_mini_batch_final_java_method_name.jsonl\n","human_eval/open_ai_call/output_mini_batch_final_python_method_name.jsonl\n"]}],"source":["# xxx\n","import pandas as pd \n","import json \n","lang = 'java'\n","\n","def parser(file):\n","    result = list()\n","    with open(file) as f:\n","        for l in f.readlines():\n","            obj = json.loads(l.strip())\n","            res = obj['response']['body']['choices'][0]['message']['content']\n","            custom_id = obj['custom_id'].split(\"#####\")\n","            # assert len(custom_id) == 3            \n","            task_id,name,partition = custom_id[0],custom_id[1], custom_id[2]\n","            result.append({'task_id': task_id,'original_method':name,\n","                           'response': res, 'partition':partition})\n","    return result \n","\n","# human_eval/open_ai_call/output_mini_batch_final_java_method_name.jsonl\n","# human_eval/open_ai_call/output_mini_batch_final_java_output.jsonl\n","# human_eval/open_ai_call/output_mini_batch_final_java_summarize.jsonl \n","langs = ['java','python']\n","for lang in langs:\n","    file = f\"human_eval/open_ai_call/output_mini_batch_final_{lang}_method_name.jsonl\"\n","    data = parser(file)\n","    df = pd.DataFrame(data)\n","    df.to_csv(f'human_eval/{lang}/method_name_prd_{lang}.csv',index=False)\n","    print(file)\n","            "]},{"cell_type":"markdown","metadata":{},"source":["GENERATE SUMMARY"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["human_eval/open_ai_call/output_mini_batch_final_java_summarize.jsonl\n","human_eval/open_ai_call/output_mini_batch_final_python_summarize.jsonl\n"]}],"source":["# xxx\n","import pandas as pd \n","import json \n","lang = 'java'\n","\n","def parser(file):\n","    result = list()\n","    with open(file) as f:\n","        for l in f.readlines():\n","            obj = json.loads(l.strip())\n","            res = obj['response']['body']['choices'][0]['message']['content']\n","            custom_id = obj['custom_id'].split(\"#####\")\n","            # assert len(custom_id) == 3      \n","            task_id,partition = custom_id[0],custom_id[1]\n","            result.append({'task_id': task_id, 'response': res, 'partition':partition})\n","    return result \n","            \n","langs = ['java','python']\n","for lang in langs:\n","    file = f\"human_eval/open_ai_call/output_mini_batch_final_{lang}_summarize.jsonl\"\n","    data = parser(file)\n","    df = pd.DataFrame(data)\n","    df.to_csv(f'human_eval/{lang}/summarize_{lang}.csv',index=False)\n","    print(file)"]},{"cell_type":"markdown","metadata":{},"source":["GENERATE OUTPUT"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["human_eval/open_ai_call/output_mini_batch_final_java_output.jsonl\n","human_eval/open_ai_call/output_mini_batch_final_python_output.jsonl\n"]}],"source":["# xxx\n","import pandas as pd \n","import json \n","lang = 'java'\n","\n","def parser(file):\n","    result = list()\n","    with open(file) as f:\n","        for l in f.readlines():\n","            obj = json.loads(l.strip())\n","            res = obj['response']['body']['choices'][0]['message']['content']\n","            custom_id = obj['custom_id'].split(\"#####\")\n","            # assert len(custom_id) == 3      \n","            task_id,test_input, expected_output, partition = custom_id[0],custom_id[1],custom_id[2],custom_id[3]\n","            result.append({'task_id': task_id, 'response': res, \n","                           'partition':partition,\n","                           'test input': test_input,\n","                           \"expected output\": expected_output})\n","    return result \n","            \n","langs = ['java','python']\n","for lang in langs:\n","    file = f\"human_eval/open_ai_call/output_mini_batch_final_{lang}_output.jsonl\"\n","    data = parser(file)\n","    df = pd.DataFrame(data)\n","    df.to_csv(f'human_eval/{lang}/output_{lang}.csv',index=False)\n","    print(file)"]},{"cell_type":"markdown","metadata":{},"source":["DRIVE"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30646,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"}},"nbformat":4,"nbformat_minor":4}
