{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from openai import OpenAI\n",
    "import os \n",
    "os.environ['OPENAI_API_KEY'] = ''\n",
    "# client = OpenAI(\n",
    "#   organization='org-T4ew5eiTxSpJrQVcBLLZQhTF',\n",
    "#   project='proj_GJ4tiZkw4JfcR6nEhgZlDI39',\n",
    "# )\n",
    "client = OpenAI()\n",
    "\n",
    "batchs  = client.batches.list(limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch(id='batch_JceTuY2k6VqjuyJCeM8TftTe', completion_window='24h', created_at=1715061295, endpoint='/v1/chat/completions', input_file_id='file-t0GxnzItsJlWNko8RFIoUM8l', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-3.5-turbo in organization org-T4ew5eiTxSpJrQVcBLLZQhTF. Limit: 200,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1715147695, failed_at=1715061295, finalizing_at=None, in_progress_at=None, metadata={}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))\n",
      "Batch(id='batch_5b3wpapyO3gnxtQOwh6ZI67T', completion_window='24h', created_at=1714925635, endpoint='/v1/chat/completions', input_file_id='file-3v6lcbEyMWptOUg7ea78zlNE', object='batch', status='completed', cancelled_at=None, cancelling_at=None, completed_at=1714926019, error_file_id=None, errors=None, expired_at=None, expires_at=1715012035, failed_at=None, finalizing_at=1714926018, in_progress_at=1714925637, metadata={}, output_file_id='file-AQ5alMiZNTbicudhMbBUWFHP', request_counts=BatchRequestCounts(completed=5, failed=0, total=5))\n",
      "Batch(id='batch_EkTu0CvgkhptI2ULuqoUbM1V', completion_window='24h', created_at=1714923738, endpoint='/v1/chat/completions', input_file_id='file-vAHOlxwAp9MGWZNMGwP1rSUf', object='batch', status='completed', cancelled_at=None, cancelling_at=None, completed_at=1714925636, error_file_id=None, errors=None, expired_at=None, expires_at=1715010138, failed_at=None, finalizing_at=1714925636, in_progress_at=1714923740, metadata={}, output_file_id='file-WrTDsl2nfY1X1MnlgQdMJIZU', request_counts=BatchRequestCounts(completed=6, failed=0, total=6))\n"
     ]
    }
   ],
   "source": [
    "for batch in batchs.data:\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<openai._legacy_response.HttpxBinaryResponseContent at 0x21a4b420e60>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_file = client.files.content('file-AQ5alMiZNTbicudhMbBUWFHP').write_to_file(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob \n",
    "import os\n",
    "import time\n",
    "import threading\n",
    "from pathlib import Path\n",
    "\n",
    "for file in glob.glob('patch_api\\cross\\patch_batch_cross_project*.jsonl'):\n",
    "    output_file = f'output_{file}'\n",
    "    if os.path.exists(output_file):\n",
    "        continue\n",
    "    file_openai = client.files.create(\n",
    "        file=Path(file),\n",
    "        purpose=\"chat\",\n",
    "    )\n",
    "    file_id = file_openai.id \n",
    "    tmp_batch = client.batches.create(\n",
    "        input_file_id= file_id,\n",
    "        endpoint=\"/v1/chat/completions\",\n",
    "        completion_window=\"24h\",\n",
    "        metadata={\n",
    "        \"description\": \"nightly eval job\"\n",
    "            }\n",
    "    )\n",
    "    batch_id = tmp_batch.id\n",
    "    while True:\n",
    "        time.sleep(1000*60) #check status batch every 60s\n",
    "        try:\n",
    "            batch = client.batches.retrieve(batch_id)\n",
    "            if batch.status == 'completed':\n",
    "                client.files.content(batch.output_file_id).write_to_file(output_file)\n",
    "                print('='*33)\n",
    "                print(f'done: {file}')\n",
    "                print(f'output: {output_file}')\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
