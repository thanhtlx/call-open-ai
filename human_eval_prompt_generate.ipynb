{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-05-06T08:26:35.626050Z","iopub.status.busy":"2024-05-06T08:26:35.625431Z","iopub.status.idle":"2024-05-06T08:27:10.664993Z","shell.execute_reply":"2024-05-06T08:27:10.663768Z","shell.execute_reply.started":"2024-05-06T08:26:35.626017Z"},"trusted":true},"outputs":[],"source":["!git clone https://github.com/thanhtlx/deepseek-eval\n","!pip install -r deepseek-eval/requirements.txt > tmp\n","!pip install pandas fire > tmp\n","import os\n","os.environ['TOKENIZERS_PARALLELISM']='false'\n","os.chdir('deepseek-eval/Evaluation/HumanEval')\n","!mkdir tmp_dir"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-06T08:27:10.667358Z","iopub.status.busy":"2024-05-06T08:27:10.667066Z","iopub.status.idle":"2024-05-06T08:27:11.797837Z","shell.execute_reply":"2024-05-06T08:27:11.796656Z","shell.execute_reply.started":"2024-05-06T08:27:10.667328Z"},"trusted":true},"outputs":[],"source":["!cd ../..  && git pull"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-06T08:27:15.220369Z","iopub.status.busy":"2024-05-06T08:27:15.219791Z","iopub.status.idle":"2024-05-06T08:27:32.446329Z","shell.execute_reply":"2024-05-06T08:27:32.445327Z","shell.execute_reply.started":"2024-05-06T08:27:15.220331Z"},"trusted":true},"outputs":[],"source":["from utils.dataset import HumanEvalDataset\n","data_root ='data/'\n","language = 'python'\n","dataset = HumanEvalDataset(data_root, language=language)\n"]},{"cell_type":"markdown","metadata":{},"source":["GENERATE CODE"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-06T08:27:32.457156Z","iopub.status.busy":"2024-05-06T08:27:32.456866Z","iopub.status.idle":"2024-05-06T08:27:35.650287Z","shell.execute_reply":"2024-05-06T08:27:35.649164Z","shell.execute_reply.started":"2024-05-06T08:27:32.457133Z"},"trusted":true},"outputs":[],"source":["from tqdm import tqdm\n","result = list()\n","PROMPT_TEMPLATE = \"\"\"\n","I want you to act as a program developer.\n","Do not write any explanations or other words, just reply with the complete function.\n","%s\n","\"\"\"\n","queue_api = list()\n","for idx in tqdm(range(0, len(dataset))):\n","    data = dataset[idx]\n","    prompt = PROMPT_TEMPLATE.strip() % data['prompt']\n","    post_api = {\"custom_id\":data['task_id'],\n","                \"method\": \"POST\",\n","                \"url\": \"/v1/chat/completions\",\n","                \"body\": {\n","                \"model\": \"gpt-3.5-turbo\",\n","                \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n","                \"temperature\": 0,\n","                \"max_tokens\": 256}}\n","    queue_api.append(post_api)\n","    print(data['prompt'])\n","    print(\"------------\")\n","import json\n","with open('queue_api_xai.jsonl','w+') as f:\n","    for obj in queue_api:\n","        f.writelines(json.dumps(obj)+\"\\n\")"]},{"cell_type":"markdown","metadata":{},"source":["GERERATE DEPENDENCE"]},{"cell_type":"code","execution_count":53,"metadata":{},"outputs":[],"source":["\n","# xxx\n","import pandas as pd \n","import json \n","\n","file = 'Human_eval_chatgpt_java.csv'\n","lang = 'java'\n","df = pd.read_csv(file)\n","result_data = list()\n","result_control = list()\n","\n","def get_post_apt(source_content,type_dependence,task_id):\n","    prompt = f\"List all pairs of statements by line numbers which have {type_dependence} relationship in the given source code: \\n\"\"\" + source_content\n","    post = {\"custom_id\":task_id,\n","                \"method\": \"POST\",\n","                \"url\": \"/v1/chat/completions\",\n","                \"body\": {\n","                \"model\": \"gpt-3.5-turbo-0125\",\n","                \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n","                \"temperature\": 0,\n","                \"max_tokens\": 512}}\n","    return post\n","\n","for _,row in df.iterrows():\n","    source_content = row['function']\n","    task_id = row['task_id']+\"#####data_dep\"\n","    result_data.append(get_post_apt(source_content,\"data dependence\",task_id))\n","    task_id = row['task_id']+\"#####control_dep\"\n","    result_control.append(get_post_apt(source_content,\"control dependence\",task_id))\n","\n","with open(f'human_eval\\\\{lang}\\\\'+f\"{lang}_data_dep.jsonl\",'w+') as f:\n","    for obj in result_data:\n","        f.writelines(json.dumps(obj) + '\\n')\n","with open(f'human_eval\\\\{lang}\\\\'+f\"{lang}_control_dep.jsonl\",'w+') as f:\n","    for obj in result_control:\n","        f.writelines(json.dumps(obj) + '\\n')"]},{"cell_type":"markdown","metadata":{},"source":["GENERATE NAME PRD"]},{"cell_type":"code","execution_count":55,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\"['lineNumber'] not in index\"\n","HumanEval_129_minPath cannot unpack non-iterable NoneType object\n","minPath\n","\"['lineNumber'] not in index\"\n","HumanEval_39_prime_fib cannot unpack non-iterable NoneType object\n","primeFib\n","\"['lineNumber'] not in index\"\n","HumanEval_117_select_words cannot unpack non-iterable NoneType object\n","selectWords\n","\"['lineNumber'] not in index\"\n","HumanEval_68_pluck cannot unpack non-iterable NoneType object\n","pluck\n","HumanEval_147_get_max_triples Incorrect Type Assumption.\n","getMaxTriples\n","\"['lineNumber'] not in index\"\n","HumanEval_47_median cannot unpack non-iterable NoneType object\n","median\n","\"['lineNumber'] not in index\"\n","HumanEval_127_intersection cannot unpack non-iterable NoneType object\n","intersection\n","\"['lineNumber'] not in index\"\n","HumanEval_152_compare cannot unpack non-iterable NoneType object\n","compare\n","\"['lineNumber'] not in index\"\n","HumanEval_135_can_arrange cannot unpack non-iterable NoneType object\n","canArrange\n","\"['lineNumber'] not in index\"\n","HumanEval_149_sorted_list_sum cannot unpack non-iterable NoneType object\n","sortedListSum\n","\"['lineNumber'] not in index\"\n","HumanEval_99_closest_integer cannot unpack non-iterable NoneType object\n","closestInteger\n","\"['lineNumber'] not in index\"\n","HumanEval_162_string_to_md5 cannot unpack non-iterable NoneType object\n","stringToMd5\n","\"['lineNumber'] not in index\"\n","HumanEval_81_numerical_letter_grade cannot unpack non-iterable NoneType object\n","numericalLetterGrade\n"]}],"source":["\n","# xxx\n","import pandas as pd \n","import json\n","from utils2 import get_node_edges\n","file = 'Human_eval_chatgpt_python.csv'\n","lang = 'python'\n","file = 'Human_eval_chatgpt_java.csv'\n","lang = 'java'\n","df = pd.read_csv(file)\n","result = list()\n","\n","def get_post_apt(source_content,index):\n","    prompt = \"Generate the function name for the given function: \\n\"\"\" + source_content\n","    post = {\"custom_id\":index,\n","                \"method\": \"POST\",\n","                \"url\": \"/v1/chat/completions\",\n","                \"body\": {\n","                \"model\": \"gpt-3.5-turbo-0125\",\n","                \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n","                \"temperature\": 0,\n","                \"max_tokens\": 256}}\n","    return post\n","fields = ['function','RenameVariable','ForWhileConvert','ReverseIf','ReorderParameter','ChangeCondition','RemoveConditionStatement','ChangeOperator','RemoveDefStatement']\n","import re\n","for idx,row in df.iterrows():\n","    node_content = df.at[idx, \"nodes\"]\n","    edge_content = df.at[idx, \"edges\"]\n","    source_content = row['function']\n","    method_name = \"\"\n","    try:\n","        if lang == 'java':\n","            nodes, edges = get_node_edges(source_content, edge_content, node_content)\n","            nodes = nodes[nodes[\"_label\"] == \"METHOD\"]\n","            method_name = nodes[nodes[\"code\"].str.contains(\"public static\")][\"name\"].tolist()[0]\n","        else:\n","            nodes = json.loads(row['nodes'])\n","            tmp = [n for n in nodes if n[\"_label\"] == \"METHOD\" and '<' not in n['name']]\n","            if len(tmp) > 0:\n","                method_name = tmp[0]['name']\n","            else:\n","                method_name = '<UKN>'\n","        \n","    except Exception as e:\n","        print(row['task_id'],e)\n","        if lang == \"java\":\n","            for lineCode in source_content.splitlines():\n","                if 'public static' in lineCode:\n","                    res = re.findall(r'\\s([A-z0-9_]+)\\(',lineCode)\n","                    if len(res) > 0 :\n","                        method_name = res[0]\n","                        print(method_name)\n","                        break\n","        if method_name == \"\":\n","            continue\n","        \n","    for field_name in fields:\n","        source_content = df.at[idx, field_name].replace(method_name, \"<FILL>\")\n","        index = f'{row[\"task_id\"]}#####{method_name}#####{field_name}'\n","        result.append(get_post_apt(source_content,index))\n","\n","with open(f'human_eval\\\\{lang}\\\\'+f\"{lang}_method_name.jsonl\",'w+') as f:\n","    for obj in result:\n","        f.writelines(json.dumps(obj) + '\\n')"]},{"cell_type":"markdown","metadata":{},"source":["GENERATE SUMMARY"]},{"cell_type":"code","execution_count":56,"metadata":{},"outputs":[],"source":["\n","# xxx\n","import pandas as pd \n","from utils2 import get_node_edges\n","file = 'Human_eval_chatgpt_python.csv'\n","lang = 'python'\n","file = 'Human_eval_chatgpt_java.csv'\n","lang = 'java'\n","df = pd.read_csv(file)\n","result = list()\n","\n","def get_post_apt(source_content,index):\n","    prompt = \"Summarize the given source code: \\n\"\"\" + source_content\n","    post = {\"custom_id\":index,\n","                \"method\": \"POST\",\n","                \"url\": \"/v1/chat/completions\",\n","                \"body\": {\n","                \"model\": \"gpt-3.5-turbo-0125\",\n","                \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n","                \"temperature\": 0,\n","                \"max_tokens\": 512}}\n","    return post\n","fields = ['function','RenameVariable','ForWhileConvert','ReverseIf','ReorderParameter','ChangeCondition','RemoveConditionStatement','ChangeOperator','RemoveDefStatement']\n","\n","for idx,row in df.iterrows():\n","    for field_name in fields:\n","        source_content = df.at[idx, field_name]\n","        index = f'{row[\"task_id\"]}#####{field_name}'\n","        result.append(get_post_apt(source_content,index))\n","\n","with open(f'human_eval\\\\{lang}\\\\'+f\"{lang}_summarize.jsonl\",'w+') as f:\n","    for obj in result:\n","        f.writelines(json.dumps(obj) + '\\n')"]},{"cell_type":"markdown","metadata":{},"source":["GENERATE OUTPUT"]},{"cell_type":"code","execution_count":57,"metadata":{},"outputs":[],"source":["\n","# xxx\n","import pandas as pd \n","from utils2 import get_node_edges\n","from redbaron import RedBaron\n","\n","file = 'Human_eval_chatgpt_python.csv'\n","lang = 'python'\n","file = 'Human_eval_chatgpt_java.csv'\n","lang = 'java'\n","df = pd.read_csv(file)\n","result = list()\n","\n","def get_post_apt(source_content,tin,index):\n","    prompt =  \"\"\"Given the function: \\n\"\"\" + source_content + \"\\n Please complete the following test case: \" + tin +\":\"\n","    post = {\"custom_id\":index,\n","                \"method\": \"POST\",\n","                \"url\": \"/v1/chat/completions\",\n","                \"body\": {\n","                \"model\": \"gpt-3.5-turbo-0125\",\n","                \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n","                \"temperature\": 0,\n","                \"max_tokens\": 256}}\n","    return post\n","fields = ['function','RenameVariable','ForWhileConvert','ReverseIf','ReorderParameter','ChangeCondition','RemoveConditionStatement','ChangeOperator','RemoveDefStatement']\n","\n","for idx,row in df.iterrows():\n","    source_content = row['function']\n","    code = row['code']\n","    test_cases = list()\n","    for line in code.splitlines():\n","        if lang == \"java\":\n","        \n","            if line.strip().startswith(\"assert\"):\n","                # print(line)\n","                if \"==\" in line:\n","                    testinput = line.split(\"==\")[0]\n","                    testoutput = line.split(\"==\")[1]\n","                elif \"equals\" in line:\n","                    testinput = line.split(\".equals\")[0]\n","                    testoutput = line.split(\".equals\")[1]\n","                test_cases.append((line.replace(testoutput, \" <FILL> \"), testoutput))\n","        else:\n","            if line.strip().startswith(\"assert\"):\n","                try:\n","                    red = RedBaron(line.strip())\n","                    testinput = red[0].value.first.dumps()\n","                    testoutput = red[0].value.second.dumps()\n","                    test_cases.append((line.replace(testoutput, \" <FILL> \"), testoutput))\n","                except Exception as e:\n","                    print(e) \n","    for field_name in fields:\n","        source_content = df.at[idx, field_name].replace(method_name, \"<FILL>\")\n","        for (tin, tout) in test_cases:\n","            index = f'{row[\"task_id\"]}#####{tin}#####{tout}#####{field_name}'\n","            result.append(get_post_apt(source_content,tin,index))\n","\n","with open(f'human_eval\\\\{lang}\\\\'+f\"{lang}_output.jsonl\",'w+') as f:\n","    for obj in result:\n","        f.writelines(json.dumps(obj) + '\\n')"]},{"cell_type":"code","execution_count":61,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["human_eval\\open_ai_call\\mini_batch_0_java_control_dep.jsonl\n","human_eval\\open_ai_call\\mini_batch_0_java_data_dep.jsonl\n","human_eval\\open_ai_call\\mini_batch_0_java_method_name.jsonl\n","human_eval\\open_ai_call\\mini_batch_0_java_output.jsonl\n","human_eval\\open_ai_call\\mini_batch_0_java_summarize.jsonl\n","human_eval\\open_ai_call\\mini_batch_0_python_control_dep.jsonl\n","human_eval\\open_ai_call\\mini_batch_0_python_data_dep.jsonl\n","human_eval\\open_ai_call\\mini_batch_0_python_method_name.jsonl\n","human_eval\\open_ai_call\\mini_batch_0_python_output.jsonl\n","human_eval\\open_ai_call\\mini_batch_0_python_summarize.jsonl\n","human_eval\\open_ai_call\\mini_batch_10_java_output.jsonl\n","human_eval\\open_ai_call\\mini_batch_11_java_output.jsonl\n","human_eval\\open_ai_call\\mini_batch_12_java_output.jsonl\n","human_eval\\open_ai_call\\mini_batch_13_java_output.jsonl\n","human_eval\\open_ai_call\\mini_batch_1_java_method_name.jsonl\n","human_eval\\open_ai_call\\mini_batch_1_java_output.jsonl\n","human_eval\\open_ai_call\\mini_batch_1_java_summarize.jsonl\n","human_eval\\open_ai_call\\mini_batch_1_python_output.jsonl\n","human_eval\\open_ai_call\\mini_batch_2_java_output.jsonl\n","human_eval\\open_ai_call\\mini_batch_2_python_output.jsonl\n","human_eval\\open_ai_call\\mini_batch_3_java_output.jsonl\n","human_eval\\open_ai_call\\mini_batch_3_python_output.jsonl\n","human_eval\\open_ai_call\\mini_batch_4_java_output.jsonl\n","human_eval\\open_ai_call\\mini_batch_4_python_output.jsonl\n","human_eval\\open_ai_call\\mini_batch_5_java_output.jsonl\n","human_eval\\open_ai_call\\mini_batch_5_python_output.jsonl\n","human_eval\\open_ai_call\\mini_batch_6_java_output.jsonl\n","human_eval\\open_ai_call\\mini_batch_6_python_output.jsonl\n","human_eval\\open_ai_call\\mini_batch_7_java_output.jsonl\n","human_eval\\open_ai_call\\mini_batch_7_python_output.jsonl\n","human_eval\\open_ai_call\\mini_batch_8_java_output.jsonl\n","human_eval\\open_ai_call\\mini_batch_8_python_output.jsonl\n","human_eval\\open_ai_call\\mini_batch_9_java_output.jsonl\n"]}],"source":["import glob \n","import json \n","for file in glob.glob(\"human_eval\\\\open_ai_call\\\\mini_batch*.jsonl\"):\n","    print(file)\n","    result = list()\n","    mapping = dict()\n","    with open(file) as f:\n","        idx = 0\n","        for line in f.readlines():\n","            idx += 1\n","            obj = json.loads(line.strip())\n","            mapping[str(idx)] = obj['custom_id']\n","            obj['custom_id'] = str(idx)\n","            result.append(obj)\n","    with open(file,'w+') as ff:\n","        for obj in result:\n","            ff.writelines(json.dumps(obj) +'\\n')\n","    with open(file+\".mapping.json\",'w+') as f:\n","        json.dump(mapping,f)\n","    "]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30646,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"}},"nbformat":4,"nbformat_minor":4}
