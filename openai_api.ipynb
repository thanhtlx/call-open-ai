{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5952\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "df = pd.read_parquet('cmg-data-v2\\\\cmg-data-cleaned.parquet',engine='fastparquet')\n",
    "test_index = list()\n",
    "with open('cmg-data-v2\\\\split-data\\\\cross_project\\\\test_id.txt') as f:\n",
    "    test_index  =[l.strip() for l in f.readlines()]\n",
    "df = df[df['index'].isin(test_index)]\n",
    "df.shape\n",
    "import re\n",
    "results = list()\n",
    "for idx in test_index:\n",
    "    source_seq = ''\n",
    "    target_seq = ''\n",
    "    for _, row in df[df['index']==idx].iterrows():\n",
    "        diffs = list()\n",
    "        for l in row['diff'].splitlines():\n",
    "            l = re.sub('@@.+?@@', '', l)\n",
    "            l = re.sub(r'\\s+', ' ', l)\n",
    "            if len(l) <= 0:\n",
    "                continue\n",
    "            words = l.split()\n",
    "            diffs.append(' '.join(words))\n",
    "            \n",
    "        if row['old_path_file'] != None:\n",
    "            old_f = row['old_path_file'].split()\n",
    "            source_seq += 'mmm ' + ' '.join(old_f) + ' <s> '\n",
    "        \n",
    "        if row['new_path_file'] != None:\n",
    "            new_f = row['new_path_file'].split()\n",
    "            source_seq += 'ppp ' + ' '.join(new_f) + ' <s> '\n",
    "        \n",
    "        source_seq += ' \\n '.join(diffs)\n",
    "        label_words = row['label'].split()\n",
    "        target_seq = ' '.join(label_words)\n",
    "    results.append({'index':idx,'source':source_seq,'label':target_seq})\n",
    "\n",
    "print(len(results))\n",
    "import json\n",
    "with open('test_cross_project_patch.jsonl','w+') as f:\n",
    "    for obj in results:\n",
    "        f.writelines(json.dumps(obj)+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "with open('test_cross_project_patch.jsonl') as f:\n",
    "    source = list()\n",
    "    for l in f.readlines():\n",
    "        obj = json.loads(l.strip())\n",
    "        source.append(obj)\n",
    "output_dir = 'data/patch_openai'\n",
    "queue_api = list()\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "I want you to act as a commit message generator.\n",
    "Do not write any explanations or other words, just reply with the commit message.\n",
    "Please generate the message of this changes:\n",
    "%s\n",
    "\"\"\"\n",
    "for obj in source:\n",
    "    prompt = PROMPT_TEMPLATE.strip() % obj['source']\n",
    "    post_api = {\"custom_id\":obj['index'],\n",
    "                \"method\": \"POST\",\n",
    "                \"url\": \"/v1/chat/completions\",\n",
    "                \"body\": {\n",
    "                \"model\": \"gpt-3.5-turbo-0125\",\n",
    "                \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "                \"temperature\": 0,\n",
    "                \"max_tokens\": 256}}\n",
    "    queue_api.append(post_api)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('queue_api_patch_cross_project.jsonl','w+') as f:\n",
    "    for obj in queue_api:\n",
    "        f.writelines(json.dumps(obj)+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SPLIT batch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "java_control_dep\n",
      "158\n",
      "java_data_dep\n",
      "158\n",
      "java_method_name\n",
      "969\n",
      "453\n",
      "java_output\n",
      "914\n",
      "806\n",
      "740\n",
      "743\n",
      "691\n",
      "876\n",
      "652\n",
      "537\n",
      "778\n",
      "764\n",
      "804\n",
      "801\n",
      "782\n",
      "30\n",
      "java_summarize\n",
      "983\n",
      "439\n",
      "python_control_dep\n",
      "164\n",
      "python_data_dep\n",
      "164\n",
      "python_method_name\n",
      "1476\n",
      "python_output\n",
      "1433\n",
      "1459\n",
      "1112\n",
      "1208\n",
      "1230\n",
      "870\n",
      "938\n",
      "1149\n",
      "447\n",
      "python_summarize\n",
      "1476\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "import json\n",
    "MAX = 170000\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "file_prefix = 'mini_batch'\n",
    "\n",
    "import glob\n",
    "for file_process in glob.glob('human_eval\\\\open_ai_call\\\\*.jsonl'):\n",
    "    # file_process = 'human_eval\\\\open_ai_call\\\\java_method_name.jsonl'\n",
    "    file_name = file_process.split(\"\\\\\")[-1].split('.')[0]\n",
    "    print(file_name)\n",
    "    index = 0\n",
    "    count = 0\n",
    "    result = list()\n",
    "    root = \"human_eval\\\\open_ai_call\\\\\"\n",
    "    with open(file_process) as f:\n",
    "        for line in f.readlines():\n",
    "            obj = json.loads(line.strip())\n",
    "            content = obj['body']['messages'][0]['content']\n",
    "            tokens = tokenizer.tokenize(content)\n",
    "            if count + len(tokens) >MAX:\n",
    "                with open(f\"{root}{file_prefix}_{index}_{file_name}.jsonl\",'w+') as ff:\n",
    "                    for tmp in result:\n",
    "                        ff.writelines(json.dumps(tmp) + '\\n')\n",
    "                print(len(result))\n",
    "                count = len(tokens)\n",
    "                result = [obj]\n",
    "                index += 1\n",
    "            else:\n",
    "                result.append(obj)\n",
    "                count += len(tokens)\n",
    "    with open(f\"{root}{file_prefix}_{index}_{file_name}.jsonl\",'w+') as ff:\n",
    "        for tmp in result:\n",
    "            ff.writelines(json.dumps(tmp) + '\\n')\n",
    "    print(len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
